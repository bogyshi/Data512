# Data 512 week 8 reading
Readings
- Read and reflect: Improving Fairness in Machine Learning Systems: What do Practitioners need

# Notes:

"Head of diversity analytics". Unsure if ive seen a role like this before.

incentivize“in-the-wild” product usage within specific
populations, such as targeted gamification techniques
This sounds strange to me. We are going to make sure groups that dont use our product often are more incentivized than others to use it so we can collect more data on them?

“Users right now are seeing [image search] as ‘We show
you [an objective] window into [...] society,’ whereas we
do have a strong argument [instead] for, ‘We should
show you as many different types of images as possible,
[to] try to get something for everyone.”
I think this is a good debate, is it better to try and reflect society as we believe it to be? Or try and overrepresent an ideallic one?

The team resolved the issue by replacing the system
outputs “nurse” and “doctor” with the more generic “health-
care professional.”
not bad

In some cases, the best option available to a
team may be to refrain from applying a particular fairness
intervention (e.g., to avoid greater harms to users)
good point

“paraphrase [an essay] in
another subgroup’s style [...] a different voice [or] vernacular
[...without] chang[ing] the linguistic content otherwise... and
say, ‘If you apply this linguistic feature, do the scores change?’
idk how i feel about this
# Reflection

## How does this reading inform your understanding of human centered data science?

The authors of this paper make a solid attempt to better understand what is missing from the toolkit of AI and ML experts in their product creation and development so as to avoid unfairness in their usage or application. They have some good insights, such as increased collaboration or documentation on attempts made by various groups on different products to address fairness, and their successful, and potentially more useful, negative results. Some observations are not as realistic unfortunately. For example, knowing the amount of data points necessary to correct for unfairness seems like a potentially unsolvable problem. It would require first knowing what you are missing, which in of itself is a tough problem, and then somehow concretely defining fairness and then determining the number. However, overall the paper provides some concrete statements and summaries of the general tools necessary for ML Practitioners to better address unfairness

## One question that came to mind.

I think back to Ishna's point last week in how this field is still quite young and to some extent that is why we are getting the results we see today. Much like the early 2000s with strange advertisements for varieties of ketchup that have not aged well, we may be in a similar period for ML models. These may likely be the days of youth that we look upon with embarrassment and disdain, but will recognize as a period of excellent growth in our personality. Perhaps that is optimistic on the field as a whole, but perhaps it is a proper analogy that reminds us to limit expectations and limit its usage in large scale systems that can severely impact society. My question is, are we too far invested in ML and AI usage in many systems to limit our expectations in this matter, or is the momentum carrying us into a future that might be too soon?
